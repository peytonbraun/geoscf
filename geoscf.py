{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060a0856-ea7b-48eb-8972-ae32d2023e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import requests\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05a8baa-a96b-4525-b9ce-646f65ec1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEOSCF:\n",
    "    def __init__(self, base_url='https://portal.nccs.nasa.gov/datashare/gmao/geos-cf/v1/das/', \n",
    "                 output_dir_pattern='GEOS_CF/Y%Y/M%m/D%d'):\n",
    "        '''\n",
    "        base_url:\n",
    "            like https://portal.nccs.nasa.gov/datashare/gmao/geos-cf/v1/ana/\n",
    "        output_dir_pattern:\n",
    "            datetime pattern of dir structure to save data\n",
    "        '''\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.base_url = base_url.rstrip('/') #if it has a trailing '/', remove it\n",
    "        self.output_dir_pattern = output_dir_pattern\n",
    "    \n",
    "    def download_and_resave(self,if_download=True,if_delete=True,\n",
    "                            generate_filenames_kw=None,\n",
    "                            download_file_kw=None,\n",
    "                            west=-170.,east=-30,south=10.,north=85.,\n",
    "                            lev_slice_collection=None,\n",
    "                            variables_collection=None,\n",
    "                            max_workers=None\n",
    "                           ):\n",
    "        '''workhorse, (optionally) download by file, load, subset, and save\n",
    "        if_download:\n",
    "            download if true\n",
    "        if_delete:\n",
    "            delete raw file if true\n",
    "        generate_filenames_kw:\n",
    "            dict, keyword arguments to function generate_filenames\n",
    "        download_file_kw:\n",
    "            input to download_file. default is good\n",
    "        wesn:\n",
    "            lon/lat boundaries\n",
    "        lev_slice_collection:\n",
    "            a list of None or slice. if a list, shall be the same length as collections\n",
    "            input to generate_filenames\n",
    "        variables_collection:\n",
    "            a list of None or list of variable names. same length as collections\n",
    "        '''\n",
    "        generate_filenames_kw = generate_filenames_kw or {}\n",
    "        download_file_kw = download_file_kw or {}\n",
    "        filenames,timestamps,collections = self.generate_filenames(**generate_filenames_kw)\n",
    "        \n",
    "        if lev_slice_collection is None:\n",
    "            lev_slice_collection = [None for _ in range(len(collections))]\n",
    "        if variables_collection is None:\n",
    "            variables_collection = [None for _ in range(len(collections))]\n",
    "\n",
    "        if if_download:\n",
    "            all_fns = filenames.flatten().tolist()\n",
    "            self.download_files_parallel_with_retry(all_fns, max_workers, **download_file_kw)\n",
    "\n",
    "        for itime,timestamp in enumerate(timestamps):\n",
    "            for ic,collection in enumerate(collections):\n",
    "                fn = filenames[itime,ic]\n",
    "                local_path = os.path.join(timestamp.strftime(self.output_dir_pattern), fn)\n",
    "            \n",
    "                ds = self.load_dataset(local_path, west=west, east=east, south=south, north=north,\n",
    "                                       lev_slice=lev_slice_collection[ic],\n",
    "                                       variables=variables_collection[ic])\n",
    "                if ds is None:\n",
    "                    self.logger.warning(f\"Skipping file due to failed load: {local_path}\")\n",
    "                    continue\n",
    "                save_fn = fn.replace('g1440x721',\n",
    "                                     'g{}x{}'.format(ds.sizes['lon'],ds.sizes['lat']))\n",
    "                # not very decent solution\n",
    "                if 'v36' in save_fn:\n",
    "                    save_fn = save_fn.replace('v36','v{}'.format(ds.sizes['lev']))\n",
    "                save_path = os.path.join(timestamp.strftime(self.output_dir_pattern),save_fn)\n",
    "                ds.to_netcdf(save_path,format='NETCDF4')\n",
    "                #self.logger.info(f\"Saving dataset to: {save_path}\")\n",
    "                ds.close() #I CHANGED THIS PART !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                if if_delete:\n",
    "                    os.remove(local_path)\n",
    "    \n",
    "    def download_file(self,fn,chunk_size=None,stream=False):\n",
    "        '''download a single file\n",
    "        keep chunk_size and stream off unless files are too big for your memory\n",
    "        '''\n",
    "        #use the base url from earlier and add the filename details\n",
    "        timestamp = dt.datetime.strptime(fn[-18:-5],\"%Y%m%d_%H%M\")\n",
    "        output_dir = timestamp.strftime(self.output_dir_pattern)\n",
    "        \n",
    "        url = \"{}/{}/{}\".format(self.base_url,timestamp.strftime('Y%Y/M%m/D%d'),fn) \n",
    "        #If there isn't a downloads folder, make one\n",
    "        os.makedirs(output_dir, exist_ok=True) \n",
    "        local_path = os.path.join(output_dir, fn)\n",
    "        self.logger.info(f\"Downloading: {url}\") \n",
    "\n",
    "        #stream download so there's no memory issues\n",
    "        response = requests.get(url, stream=stream) \n",
    "        #handle what happens if the file isn't there\n",
    "        if response.status_code == 404: \n",
    "            self.logger.warning(f\"File not found: {fn}\")\n",
    "            return local_path\n",
    "        #automatically checks if the request succeeded and if not, says what happened\n",
    "        response.raise_for_status() \n",
    "\n",
    "        #open the file, give it the name 'f'\n",
    "        with open(local_path, \"wb\") as f: \n",
    "            if chunk_size is None:\n",
    "                f.write(response.content)\n",
    "            else:\n",
    "                #basically processes the file in chunks, which is more efficient for memory\n",
    "                for chunk in response.iter_content(chunk_size=chunk_size): \n",
    "                    f.write(chunk) #building up the file piece by piece\n",
    "        self.logger.info(f\"Saved to: {local_path}\")\n",
    "        return local_path\n",
    "    \n",
    "    def download_files(self,fns,**kwargs):\n",
    "        '''download multiple files.'''\n",
    "        local_paths = []\n",
    "        for fname in fns:\n",
    "            try: #if one file fails, continues to download the rest of the files\n",
    "                local_path = self.download_file(fname, **kwargs)\n",
    "                if local_path:\n",
    "                    local_paths.append(local_path)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to download {fname}: {e}\")\n",
    "        return local_paths\n",
    "    \n",
    "    def load_dataset(self,local_path,west=-170.,east=-30,south=10.,north=85.,lev_slice=None,\n",
    "                     variables=None\n",
    "                    ):\n",
    "        '''load and subset data from a single file\n",
    "        wesn:\n",
    "            lon/lat boundaries\n",
    "        lev_slice:\n",
    "            a slice, reserved to subset vertically\n",
    "        variables:\n",
    "            variables to load\n",
    "        '''\n",
    "        #self.logger.info(f\"Loading dataset from: {local_path}\")\n",
    "        if not os.path.exists(local_path):\n",
    "            self.logger.warning(f'{local_path} does not exist!')\n",
    "            return\n",
    "        ds = xr.open_dataset(local_path)\n",
    "        if lev_slice is None:\n",
    "            indexers = dict(lon=slice(west,east),lat=slice(south,north))\n",
    "        else:\n",
    "            indexers = dict(lon=slice(west,east),lat=slice(south,north),lev=lev_slice)\n",
    "        ds_sel = ds.sel(indexers)\n",
    "        if variables is None:\n",
    "            return ds_sel.load()\n",
    "        else:\n",
    "            variables = [v for v in ds.data_vars if v in variables]\n",
    "            if not variables:\n",
    "                self.logger.warning(f\"No matching variables in {local_path}, return all\")\n",
    "                return ds_sel.load()\n",
    "            else:\n",
    "                return ds_sel[variables].load()\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_filenames(fn_template='GEOS-CF.{}.{}.{}.{}.nc4',\n",
    "                           versions=None,modes=None,\n",
    "                           collections=[\n",
    "                               'met_tavg_1hr_g1440x721_x1',\n",
    "                               'ems_tavg_1hr_g1440x721_x1'\n",
    "                           ],\n",
    "                           timestamps=None,\n",
    "                           start=None,end=None,period=None,freq=None\n",
    "                          ):\n",
    "        '''generate a list of GEOS-CF filenames based on collections and time range\n",
    "        see section 5.1 at https://gmao.gsfc.nasa.gov/pubs/docs/Knowland1446.pdf\n",
    "        versions,modes:\n",
    "            by default 'v01' and 'rpl'\n",
    "        collections:\n",
    "            lists of geos cf collections\n",
    "        return:\n",
    "            filenames as a 2d array, len(timestamps),len(collections)\n",
    "            timestamps and collections\n",
    "        '''\n",
    "        # chm_tavg_1hr_g1440x721_v36: 2.9G\n",
    "        # met_tavg_1hr_g1440x721_v36: 0.6G\n",
    "        if versions is None:\n",
    "            versions = ['v01' for _ in range(len(collections))]\n",
    "        if modes is None:\n",
    "            modes = ['rpl' for _ in range(len(collections))]\n",
    "        #if timestamps is None:\n",
    "            #timestamps = pd.date_range(start,end,period,freq)\n",
    "        if timestamps is None:\n",
    "            if start is not None and (end is not None or period is not None):\n",
    "                timestamps = pd.date_range(start=start, end=end, periods=period, freq=freq)\n",
    "            else:\n",
    "                raise ValueError(\"Must provide either 'timestamps' or both 'start' and 'end' with 'freq'\")\n",
    "        filenames = np.empty((len(timestamps),len(collections)),dtype=object)\n",
    "        \n",
    "        for ic,collection in enumerate(collections):\n",
    "            for itime,timestamp in enumerate(timestamps):\n",
    "                tstr = timestamp.strftime(\"%Y%m%d_%H%Mz\")\n",
    "                fn = fn_template.format(versions[ic],modes[ic],collection,tstr)\n",
    "                filenames[itime,ic] = fn\n",
    "        return filenames,timestamps,collections\n",
    "\n",
    "    def download_file_with_retry(self, fn, retries=5, delay=5, **kwargs):\n",
    "        '''\n",
    "        Download a single file with retry logic to handle transient errors like\n",
    "        connection timeouts or network glitches.\n",
    "    \n",
    "        Parameters:\n",
    "            fn (str): Filename to download.\n",
    "            retries (int): Number of retry attempts before giving up.\n",
    "            delay (int or float): Seconds to wait between retries.\n",
    "            **kwargs: Additional keyword arguments passed to download_file.\n",
    "    \n",
    "        Returns:\n",
    "            local_path (str or None): Path to downloaded file if successful,\n",
    "                                      None if all retries failed.\n",
    "        '''\n",
    "        for i in range(retries):\n",
    "            try:\n",
    "                # Try downloading the file normally\n",
    "                return self.download_file(fn, **kwargs)\n",
    "            except (requests.ConnectionError, requests.Timeout) as e:\n",
    "                # If a connection error or timeout occurs, log it and retry after delay\n",
    "                self.logger.warning(f\"Download failed for {fn} on attempt {i+1}/{retries}: {e}\")\n",
    "                time.sleep(delay)\n",
    "        # If all retries exhausted without success, log error and return None\n",
    "        self.logger.error(f\"All retries failed for {fn}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def download_files_parallel_with_retry(self, fns, max_workers, **kwargs):\n",
    "        '''\n",
    "        Download multiple files in parallel using threads with retry logic.\n",
    "    \n",
    "        Parameters:\n",
    "            fns (list): List of filenames to download.\n",
    "            max_workers (int): Maximum number of concurrent threads.\n",
    "            **kwargs: Additional arguments passed to download_file_with_retry.\n",
    "    \n",
    "        Returns:\n",
    "            local_paths (list): List of paths to successfully downloaded files.\n",
    "        '''\n",
    "        local_paths = []  # To store successful download paths\n",
    "        completed = 0\n",
    "    \n",
    "        # Use ThreadPoolExecutor to manage a pool of worker threads\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit download tasks to executor for each filename\n",
    "            future_to_fn = {executor.submit(self.download_file_with_retry, fn, **kwargs): fn for fn in fns}\n",
    "            # As each thread completes, process the results\n",
    "            for future in as_completed(future_to_fn):\n",
    "                fn = future_to_fn[future]\n",
    "                try:\n",
    "                    result = future.result()  # Get the result of download_file_with_retry\n",
    "                    if result:\n",
    "                        local_paths.append(result)  # Append successful downloads\n",
    "                    completed += 1\n",
    "                    self.logger.info(f\"Downloaded {completed}/{len(fns)}: {fn}\")  # Log progress\n",
    "                except Exception as e:\n",
    "                    # Log if any unexpected exceptions occur during download\n",
    "                    self.logger.warning(f\"Failed to download {fn}: {e}\")\n",
    "        return local_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reu_env",
   "language": "python",
   "name": "reu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
